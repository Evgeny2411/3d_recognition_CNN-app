{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#3d processing\n",
    "import pywavefront\n",
    "import trimesh\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:47:34.798681Z",
     "end_time": "2023-04-24T16:47:43.028616Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data collection and preparaion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collect_data(dir_path, split_name):\n",
    "    data = []\n",
    "\n",
    "    for shape in os.listdir(dir_path):\n",
    "        shape_path = os.path.join(dir_path, shape)\n",
    "        if os.path.isdir(shape_path):\n",
    "            split_path = os.path.join(shape_path, split_name)\n",
    "            if os.path.isdir(split_path):\n",
    "                file_data = [{'filename': os.path.join(split_path, file), 'label':file.split('.')[0]} for file in os.listdir(split_path) if file.endswith('.obj')]\n",
    "                data.extend(file_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "dir_path = 'data'\n",
    "train = collect_data(dir_path, 'train')\n",
    "test = collect_data(dir_path, 'test')\n",
    "valid = collect_data(dir_path, 'valid')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:47:44.759264Z",
     "end_time": "2023-04-24T16:47:44.824570Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encode data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['label'])\n",
    "train['label'] = encoder.transform(train['label'])\n",
    "test['label'] = encoder.transform(test['label'])\n",
    "valid['label'] = encoder.transform(valid['label'])\n",
    "\n",
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:47:47.184183Z",
     "end_time": "2023-04-24T16:47:47.199366Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3d visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def visualise(points):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    voxel_grid=o3d.geometry.VoxelGrid.create_from_point_cloud(pcd,voxel_size=0.01)\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name='Points', width=800, height=600)\n",
    "\n",
    "    vis.add_geometry(voxel_grid)\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "\n",
    "obj = pywavefront.Wavefront('data/torus/train/Torus.1541.obj')\n",
    "#visualise(obj.vertices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:49:10.322813Z",
     "end_time": "2023-04-24T16:49:13.722443Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset implementation for torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, df, num_points):\n",
    "        self.df = df\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df['filename'][idx]\n",
    "        label = self.df['label'][idx]\n",
    "        mesh = trimesh.load(file_path)\n",
    "        points = mesh.vertices\n",
    "        # Sample a fixed number of points from the point cloud\n",
    "        points, label = self.downsample(points, label)\n",
    "        # Normalize the point cloud\n",
    "        points = (points - np.mean(points, axis=0)) / np.std(points, axis=0)\n",
    "        points = torch.from_numpy(points).type(torch.float32)\n",
    "        label = torch.tensor(label).type(torch.LongTensor)\n",
    "        return points, label\n",
    "\n",
    "    def downsample(self, points, targets):\n",
    "        if len(points) > self.num_points:\n",
    "            choice = np.random.choice(len(points), self.num_points, replace=False)\n",
    "        else:\n",
    "            # case when there are less points than the desired number\n",
    "            choice = np.random.choice(len(points), self.num_points, replace=True)\n",
    "        points = points[choice, :]\n",
    "        targets = targets\n",
    "\n",
    "        return points, targets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:48:00.370728Z",
     "end_time": "2023-04-24T16:48:00.393707Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = PointCloudDataset(train, 1024)\n",
    "test_dataset = PointCloudDataset(test, 1024)\n",
    "valid_dataset = PointCloudDataset(valid, 1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:48:02.615466Z",
     "end_time": "2023-04-24T16:48:02.637388Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PointNet implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Tnet(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__()\n",
    "        self.k=k\n",
    "        self.conv1 = nn.Conv1d(k,64,1)\n",
    "        self.conv2 = nn.Conv1d(64,128,1)\n",
    "        self.conv3 = nn.Conv1d(128,1024,1)\n",
    "        self.fc1 = nn.Linear(1024,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,k*k)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs = input.size(0)\n",
    "        xb = F.relu(self.bn1(self.conv1(input)))\n",
    "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "        xb = F.relu(self.bn3(self.conv3(xb)))\n",
    "        pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "        flat = nn.Flatten(1)(pool)\n",
    "        xb = F.relu(self.bn4(self.fc1(flat)))\n",
    "        xb = F.relu(self.bn5(self.fc2(xb)))\n",
    "\n",
    "        init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
    "        if xb.is_cuda:\n",
    "            init=init.cuda()\n",
    "        matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
    "        return matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:48:04.447708Z",
     "end_time": "2023-04-24T16:48:04.460247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PointNetBackbone(nn.Module):\n",
    "    def __init__(self, num_points=1024, num_feats=1024):\n",
    "\n",
    "        super(PointNetBackbone, self).__init__()\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.num_feats = num_feats\n",
    "\n",
    "        # Spatial Transformer Networks (T-nets)\n",
    "        self.tnet1 = Tnet(3)\n",
    "        self.tnet2 = Tnet(64)\n",
    "\n",
    "        # shared MLP 1\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1)\n",
    "\n",
    "        # shared MLP 2\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=1)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=1)\n",
    "        self.conv5 = nn.Conv1d(128, self.num_feats, kernel_size=1)\n",
    "\n",
    "        # batch norms for both shared MLPs\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(self.num_feats)\n",
    "\n",
    "        # max pool to get the global features\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=num_points, return_indices=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        # pass through first Tnet to get transform matrix\n",
    "        A_input = self.tnet1(x)\n",
    "\n",
    "        # perform first transformation across each point in the batch\n",
    "        x = torch.bmm(x.transpose(2, 1), A_input).transpose(2, 1)\n",
    "\n",
    "        # pass through first shared MLP\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        x = self.bn2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # get feature transform\n",
    "        A_feat = self.tnet2(x)\n",
    "\n",
    "        # perform second transformation across each (64 dim) feature in the batch\n",
    "        x = torch.bmm(x.transpose(2, 1), A_feat).transpose(2, 1)\n",
    "\n",
    "\n",
    "        # pass through second MLP\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.bn5(F.relu(self.conv5(x)))\n",
    "\n",
    "        # get feature vector and critical indexes\n",
    "        features, critical_indexes = self.max_pool(x)\n",
    "        features = features.view(bs, -1)\n",
    "        critical_indexes = critical_indexes.view(bs, -1)\n",
    "\n",
    "        return features, critical_indexes, A_feat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:48:06.292145Z",
     "end_time": "2023-04-24T16:48:06.329060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, classes = 6):\n",
    "        super().__init__()\n",
    "        self.transform = PointNetBackbone()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, classes)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x, crit_idxs, A_feat = self.transform(input)\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x, crit_idxs, A_feat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:48:10.926427Z",
     "end_time": "2023-04-24T16:48:10.945032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PointNetLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=0, reg_weight=0, size_average=True):\n",
    "        super(PointNetLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reg_weight = reg_weight\n",
    "        self.size_average = size_average\n",
    "\n",
    "        if isinstance(alpha,(float, int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,(list, np.ndarray)): self.alpha = torch.Tensor(alpha)\n",
    "\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(weight=self.alpha)\n",
    "\n",
    "\n",
    "    def forward(self, predictions, targets, A=None):\n",
    "\n",
    "        bs = predictions.size(0)\n",
    "\n",
    "        ce_loss = self.cross_entropy_loss(predictions, targets)\n",
    "\n",
    "        pn = F.softmax(predictions, dim = 0)\n",
    "        pn = pn.gather(1, targets.view(-1, 1)).view(-1)\n",
    "\n",
    "        # get regularization\n",
    "        if self.reg_weight > 0:\n",
    "            I = torch.eye(64).unsqueeze(0).repeat(A.shape[0], 1, 1) # .to(device)\n",
    "            if A.is_cuda: I = I.cuda()\n",
    "            reg = torch.linalg.norm(I - torch.bmm(A, A.transpose(2, 1)))\n",
    "            reg = self.reg_weight*reg/bs\n",
    "        else:\n",
    "            reg = 0\n",
    "\n",
    "        loss = ((1 - pn)**self.gamma * ce_loss)\n",
    "        if self.size_average: return loss.mean() + reg\n",
    "        else: return loss.sum() + reg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:04:35.545009Z",
     "end_time": "2023-04-24T11:04:35.769316Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "test on one batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "points, targets = next(iter(train_loader))\n",
    "classifier = PointNet(6)\n",
    "out, _, _ = classifier(points.transpose(2,1))\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:48:53.752640Z",
     "end_time": "2023-04-24T16:48:56.343073Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 0.0001\n",
    "REG_WEIGHT = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alpha = np.ones(6)\n",
    "gamma = 2\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01,\n",
    "                                              step_size_up=2000, cycle_momentum=False)\n",
    "criterion = PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=REG_WEIGHT).to(device)\n",
    "\n",
    "classifier = classifier.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:04:39.050581Z",
     "end_time": "2023-04-24T11:04:39.138587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_test(classifier, dataloader, num_batch, epoch, split='train'):\n",
    "    _loss = []\n",
    "    _accuracy = []\n",
    "\n",
    "    total_test_targets = []\n",
    "    total_test_preds = []\n",
    "    for i, (points, targets) in enumerate(dataloader, 0):\n",
    "\n",
    "        points = points.transpose(2, 1).to(device)\n",
    "        targets = targets.squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds, _, A = classifier(points)\n",
    "        loss = criterion(preds, targets, A)\n",
    "\n",
    "        if split == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        pred_choice = torch.softmax(preds, dim=1).argmax(dim=1)\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(32)\n",
    "\n",
    "        _loss.append(loss.item())\n",
    "        _accuracy.append(accuracy)\n",
    "\n",
    "        if split == 'test':\n",
    "            total_test_targets += targets.reshape(-1).cpu().numpy().tolist()\n",
    "            total_test_preds += pred_choice.reshape(-1).cpu().numpy().tolist()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'\\t [{epoch}: {i}/{num_batch}] ' \\\n",
    "                  + f'{split} loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f}')\n",
    "\n",
    "    epoch_loss = np.mean(_loss)\n",
    "    epoch_accuracy = np.mean(_accuracy)\n",
    "\n",
    "    print(f'Epoch: {epoch} - {split} Loss: {epoch_loss:.4f} ' \\\n",
    "          + f'- {split} Accuracy: {epoch_accuracy:.4f} ' \\\n",
    "          + f'- {split}')\n",
    "\n",
    "    if split == 'test':\n",
    "        return epoch_loss, epoch_accuracy, total_test_targets, total_test_preds\n",
    "    else:\n",
    "        return epoch_loss, epoch_accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:04:39.152588Z",
     "end_time": "2023-04-24T11:04:39.196588Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_batch = int(np.ceil(len(train_dataset)/32))\n",
    "num_valid_batch = int(np.ceil(len(test_dataset)/32))\n",
    "\n",
    "train_metrics = []\n",
    "valid_metrics = []\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "\n",
    "    classifier = classifier.train()\n",
    "\n",
    "    _train_metrics = train_test(classifier, train_loader,\n",
    "                                num_train_batch, epoch,\n",
    "                                split='train')\n",
    "    train_metrics.append(_train_metrics)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        classifier = classifier.eval()\n",
    "\n",
    "        _valid_metrics = train_test(classifier, test_loader,\n",
    "                                    num_valid_batch, epoch,\n",
    "                                    split='test')\n",
    "        valid_metrics.append(_valid_metrics)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:04:39.177588Z",
     "end_time": "2023-04-24T11:09:42.268299Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "acc = []\n",
    "\n",
    "for i, (points, targets) in enumerate(valid_loader, 0):\n",
    "    with torch.no_grad():\n",
    "        classifier.eval().to('cpu')\n",
    "\n",
    "        points = points.transpose(2,1)\n",
    "        targets = targets.squeeze()\n",
    "\n",
    "        preds, _, A = classifier(points)\n",
    "        pred_choice = torch.softmax(preds, dim=1).argmax(dim=1)\n",
    "\n",
    "        correct = (pred_choice == targets.data).sum()\n",
    "        accuracy = correct.item()/float(32)\n",
    "\n",
    "        predictions.extend(pred_choice)\n",
    "        labels.extend(targets.data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:09:42.283744Z",
     "end_time": "2023-04-24T11:09:58.351670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_names = ['loss', 'accuracy']\n",
    "_, ax = plt.subplots(len(metric_names), 1, figsize=(8, 6))\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    ax[i].set_title(m)\n",
    "    ax[i].plot([t[i] for t in train_metrics], label='train')\n",
    "    ax[i].plot([t[i] for t in valid_metrics], label='test')\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.subplots_adjust(wspace=0., hspace=0.35)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:09:59.841221Z",
     "end_time": "2023-04-24T11:10:00.623917Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check accuracy and another metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(encoder.inverse_transform(labels), encoder.inverse_transform(predictions)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cf_matrix = pd.DataFrame(columns= encoder.classes_, index=encoder.classes_, data = confusion_matrix(encoder.inverse_transform(labels), encoder.inverse_transform(predictions)))\n",
    "cf_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:10:00.629914Z",
     "end_time": "2023-04-24T11:10:00.734640Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "save model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = \"models/modelv1\"\n",
    "torch.save(classifier.state_dict(), path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:12:17.301049Z",
     "end_time": "2023-04-24T11:12:17.439666Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
